{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - Data processing at scale - Introduction to Spark\n",
    "\n",
    "----\n",
    "**Group**:\n",
    " * Student Name\n",
    " * Student Name\n",
    "\n",
    "---- \n",
    "\n",
    "We'll be using the [Spark](http://spark.apache.org) framework to process and analyse some data ([Quick overview](http://spark.apache.org/docs/1.6.2/quick-start.html))\n",
    "\n",
    "The `pyspark` module provide the necessary bindings to the Spark engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dependencies - run this cell first\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import os.path\n",
    "from pandas import DataFrame\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the Spark Context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not globals().get('sc'):\n",
    "    sc = SparkContext('local', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can have a look to the functions provided by the SparkContext in this notebook using the `help()` function or [online](https://spark.apache.org/docs/1.6.2/api/python/pyspark.html#pyspark.SparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the Spark Context is started, you can have access to the UI to visualize some information:\n",
    "http://127.0.0.1:4040/ (change the ip to the correct one if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a random list of integers as a sample of data to perform some analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [random.randint(0,10) for i in range(0, 1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create a RDD from this data using the `parallelize` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(sc.parallelize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numSlices` indicates the number of partitions into which the data will be split. Each partition represent a subset of the data on which Spark will apply your transformations/processing in parallel.\n",
    "\n",
    "For instance to create 4 partitions for our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_rdd = sc.parallelize(data, 4)\n",
    "print 'df type:', type(my_rdd)\n",
    "print 'Num partitions:', my_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDD also has its own set of functions ([online](https://spark.apache.org/docs/1.6.2/api/python/pyspark.html#pyspark.RDD) documentation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(my_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply count the number of objects in the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Total number of elements:', my_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To filter elements greater than 5, fetch first 10, then count total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "greater_than_5 = my_rdd.filter(lambda x: x>=5)\n",
    "print '10 first elements greater than 5:', greater_than_5.top(10)\n",
    "print 'count of elements greater than 5:', greater_than_5.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than sorting the RDD with `top()`, we can just `take()` the first elements of the RDD (unsorted):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "greater_than_5.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine we want to know the distinct values contained in the RDD, we can use the `distinct()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_values = greater_than_5.distinct()\n",
    "d_values.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass a function as a filter rather than a lambda function if the filtering is more complex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_filter(value):\n",
    "    return value<5\n",
    "\n",
    "lower_than_5 = my_rdd.filter(my_filter)\n",
    "\n",
    "print '5 last elements lower than 5:', lower_than_5.top(5)\n",
    "print 'list of all elements lower than 5:', lower_than_5.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `collect()` returns all the elements and data can be stored in a local python variable. \n",
    "\n",
    "In case of large data sets, this can be not practical and the result can rather be stored to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('lower_than_5'):\n",
    "    lower_than_5.saveAsTextFile('lower_than_5')\n",
    "else:\n",
    "    print 'Directory already exists'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file is written to the `lower_than_5` directory with one file per partition of the RDD (`part-XXXXX`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.listdir('lower_than_5')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each `.crc` file contain a checksum of the associated file. This is for checking integrity (Cyclical Redundancy Check)\n",
    "* `_SUCCESS` means the operation completed successfully\n",
    "\n",
    "Transformations can be applied to an RDD do derive new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multiply_by_2 = my_rdd.map(lambda x: x*2)\n",
    "\n",
    "print 'Multiply by 2:', multiply_by_2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the number of occurences of each integer using the `reduceByKey` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(my_rdd.reduceByKey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept is similar to Map/Reduce, for each integer of the RDD we emit a tuple `(int, 1)`, reduceByKey will:\n",
    " * group the new RDD by its key (first element of the tuple) \n",
    " * sum up the list of values associated to this key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group_by_value = my_rdd.map(lambda x: (x, 1)).reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "print 'Group by value:'\n",
    "for el in group_by_value.collect():\n",
    "    print '\\tvalue %s: count: %s' % el "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "External data sets can be loaded, for instance we can load a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile('lower_than_5/part-00000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the number of lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Number of lines:', lines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1 - Word Count\n",
    "\n",
    "We are considering in this exercice the list of all unique artist terms (Echo Nest tags) from the [Million Song Data Set](http://labrosa.ee.columbia.edu/millionsong/pages/getting-dataset).\n",
    "\n",
    "You can download the data set at:\n",
    "\n",
    "    https://s3-eu-west-1.amazonaws.com/scimus-data/lab4/unique_terms.txt\n",
    "\n",
    "Using this data set, answer the following questions:\n",
    "\n",
    "* create a RDD from this file\n",
    "* generate a new RDD by transforming each line and splitting it to get all the words (use the [`flatMap` function](https://spark.apache.org/docs/1.6.2/api/python/pyspark.html#pyspark.RDD.flatMap))\n",
    "* count the number of words by emitting a key-value pair `(word, 1)`\n",
    "* aggregate the count by word\n",
    "* collect the top 10 by count (look at the [`takeOrdered` function](https://spark.apache.org/docs/1.6.2/api/python/pyspark.html#pyspark.RDD.takeOrdered) of a RDD)\n",
    "* display a word cloud of the data sets\n",
    "\n",
    "For the last question (Word Cloud), use the `wordcloud` library (https://github.com/amueller/word_cloud)\n",
    "* Install it in your python environment if required\n",
    "\n",
    "        pip install --user wordcloud\n",
    "\n",
    "* Generate the cloud using the method `fit_words`\n",
    "* Display the image (see http://amueller.github.io/word_cloud/auto_examples/simple.html as an example)\n",
    "        \n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2 - Data exploration 1\n",
    " \n",
    "This data set contains the playlist of about 1K lastFM users.\n",
    "\n",
    "See http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/lastfm-1K.html\n",
    "\n",
    "You can download the specific file at:\n",
    "    \n",
    "    https://s3-eu-west-1.amazonaws.com/scimus-data/lab4/lastfm-dataset-1K/userid-timestamp-artid-artname-traid-traname.tsv.gz\n",
    "    \n",
    "For instance\n",
    "\n",
    "    curl -O https://s3-eu-west-1.amazonaws.com/scimus-data/lab4/lastfm-dataset-1K/userid-timestamp-artid-artname-traid-traname.tsv.gz\n",
    "    gunzip userid-timestamp-artid-artname-traid-traname.tsv.gz\n",
    "    \n",
    "\n",
    "To simplify the testing/validation, run first your code on a smaller subset:\n",
    "\n",
    "    https://s3-eu-west-1.amazonaws.com/scimus-data/lab4/lastfm-dataset-1K/userid-timestamp-artid-artname-traid-traname-1000.tsv\n",
    "\n",
    "Once you are confident, you can run the code on the full data set.\n",
    "\n",
    "The format of this file is a TSV (Tab Separated Values) with the following structure:\n",
    "\n",
    "    userid \\t timestamp \\t musicbrainz-artist-id \\t artist-name \\t musicbrainz-track-id \\t track-name\n",
    "    \n",
    "As artist identifiers and track identifiers, you will use the `artist-name` field and the `track-name` fields (the `musicbrain-*` fields may be not provided for all rows).    \n",
    "\n",
    "For this exercice, you will:\n",
    "\n",
    "* Load the lastfm data set into Spark\n",
    "* Compute the total number of plays by `artist-name` and display the top 10.\n",
    "* Compute the total number of plays by `userid` and display the top 10.\n",
    "* Compute the top 100 tracks being played. Hint: Several artists may have created a track of the same name, they are however different tracks and should be counted independently from each others ... \n",
    "* Create a RDD containing the words in the tracks names to answer the following questions:\n",
    "  * How many distinct words in total?\n",
    "  * What is the top 10 of most frequent words?\n",
    "  * Filter out preposition, articles, ... and compute the new top 10\n",
    "  * Display a Word Cloud of all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3 - Collaborative filtering\n",
    "\n",
    "### Introduction\n",
    "https://en.wikipedia.org/wiki/Collaborative_filtering\n",
    "\n",
    "> Collaborative filtering (CF) is a technique used by some recommender systems [...]\n",
    "\n",
    "> [...] collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue x than to have the opinion on x of a person chosen randomly. For example, a collaborative filtering recommendation system for television tastes could make predictions about which television show a user should like given a partial list of that user's tastes (likes or dislikes). Note that these predictions are specific to the user, but use information gleaned from many users. This differs from the simpler approach of giving an average (non-specific) score for each item of interest, for example based on its number of votes.\n",
    "\n",
    "For this exercice we'll be using the [Last.fm Dataset - 360K users](http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/lastfm-360K.html) and build a model to recommend new artists to users based on their listening activities. \n",
    "\n",
    "A full data set and a reduced one (for testing) is available at:\n",
    "\n",
    "    https://s3-eu-west-1.amazonaws.com/scimus-data/lab4/lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv.1000\n",
    "    https://s3-eu-west-1.amazonaws.com/scimus-data/lab4/lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv.gz\n",
    "    \n",
    "These are TSV files with the following format:\n",
    "\n",
    "    user-mboxsha1 \\t musicbrainz-artist-id \\t artist-name \\t plays\n",
    "\n",
    "The Spark mlib library implements one commonly used algorithm for collaborative filtering analysis based on the Alternating Least Square method (ALS). See http://spark.apache.org/docs/1.6.2/mllib-collaborative-filtering.html for example and details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "To apply the ALS technique on the data set, we first need to convert all values to numerical fields to build a [`Rating` object](https://spark.apache.org/docs/1.6.3/api/python/pyspark.mllib.html?highlight=rating#pyspark.mllib.recommendation.Rating).\n",
    "\n",
    "By consequence, we need to assign unique integer values to users and artists (they currently are respectively SHA-1 and UUIDs - strings -)\n",
    "\n",
    "First, load the file into a RDD using the `textFile` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code\n",
    "source = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go through the data set, list all artists and assign them a unique ID.\n",
    "\n",
    "For this:\n",
    " * We write a function that given a line of the file, split it and return the tuple `(musicbrainz-artist-id, artist-name)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_artist(line):\n",
    "    data = line.split('\\t')\n",
    "    return (data[1], data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then run this function through the data set and get the list of unique artists using the `distinct()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "artists = source.map(get_artist).distinct()\n",
    "print 'Number of artists:', artists.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need later on to get the unique id for the tuple `(musicbrainz-artist-id, artist-name)`, so we build a quick index with the [`zipWithUniqueId` function](https://spark.apache.org/docs/1.6.3/api/python/pyspark.html?highlight=zipwithuniqueid#pyspark.RDD.zipWithUniqueId) and use the Spark `broadcat` feature to make it available to all the Spark workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a unique ID for each artist\n",
    "artists_with_unique_id = artists.zipWithUniqueId().collect()\n",
    "artists_with_unique_id[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build an index to map artist to its id which can be used by Spark\n",
    "artists_index = sc.broadcast(dict(zip([a[0] for a in artists_with_unique_id], [a[1] for a in artists_with_unique_id])))\n",
    "\n",
    "# artists_index is a Broadcast object\n",
    "print type(artists_index)\n",
    "# which .value property is a python dictionary\n",
    "print type(artists_index.value)\n",
    "# the dictionary contains the mapping artits --> unique integer id\n",
    "print artists_index.value.items()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn to do the same for users:\n",
    "* write a function to get the `userid` from a line of the source RDD\n",
    "* create a RDD containing all unique users\n",
    "* create a unique ID for each user using the previous technique\n",
    "* create a spark broadcast variable to distribute it to the Spark workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to parse the data set and create a new `Rating` object for each data point, this will be \n",
    "```\n",
    "Rating(user_id, artist_id, listens)\n",
    "```\n",
    "where:\n",
    "* `user_id` is the integer id we have assigned\n",
    "* `artist_id` is the integer id of the artist we have assigned\n",
    "* `listens` is the number of listens for this user and artist\n",
    "\n",
    "In order to generate the Ratings:\n",
    "* Write a function that will parse each line of the source and build the above `Rating` object.\n",
    "* Apply the function to the data set\n",
    "* Display the first 5 ratings of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting\n",
    "\n",
    "We need to split the data set in 2 different ones:\n",
    " * training\n",
    " * test\n",
    "\n",
    "To split the data set, we'll be removing random entries in the data set by sampling it. This can be done using the `randomSplit()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "We can now run the recommender on this data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the recommendation model using Alternating Least Squares with implicit ratings\n",
    "rank = 10\n",
    "numIterations = 10\n",
    "alpha = 0.01\n",
    "lambda_ = 0.01\n",
    "\n",
    "model = ALS.trainImplicit(training, rank, numIterations, alpha=alpha, lambda_=lambda_, nonnegative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "The generated model can be saved for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save(sc, \"myCollaborativeFilter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and then loaded when needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recommender = MatrixFactorizationModel.load(sc, \"myCollaborativeFilter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `recommender` object can now be used to perform some recomendations.\n",
    "\n",
    "* Pick a random user in the training set and predict the 10 most likely artists he will be interested in (`recommendProducts()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For some users in the test set, compute the list of unique artists in the data set for this user and compare against the recommendations for that user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pick a random artist and predict the 10 most likely users that will be interested by this artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We have arbitrary chosen some factors for training the model (*lambda*, *rank*, *iterations*, ...).\n",
    "\n",
    "In practice, the recommender has to be evaluated on a data set and tuned according to an evaluation metric.\n",
    "\n",
    "For simplicity we'll evaluate the model on whether or not the first top 10 recommendations are in the users playlist.\n",
    "\n",
    "* For each users, write a function that counts the number of common artists between the user's listen patterns and the recommendations\n",
    "* Compute the average number of common elements on the test data set\n",
    "* Investigate the impact of different parameters on the average value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
